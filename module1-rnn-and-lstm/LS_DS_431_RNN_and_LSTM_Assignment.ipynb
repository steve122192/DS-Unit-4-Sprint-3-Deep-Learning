{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEPzxbQReLkn",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7wgbo1bfPen",
        "colab_type": "code",
        "outputId": "9701ab9e-70bc-4096-8ce3-3ea9badf23cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Caw43JZ_faCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "with requests.get(url) as res:\n",
        "    res.encoding = 'utf-8'\n",
        "    works = res.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZstihxhhAOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unique Characters\n",
        "chars = list(set(works))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVez2kC4hSpi",
        "colab_type": "code",
        "outputId": "681d6046-553b-488c-9036-3c6bc500b033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(int_char)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "outputId": "73dda8c8-f687-47ac-e52b-927937777eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in works]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  1148003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8P0MlWVjoWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR1ci3JXjf34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vayQaDx4j7Jk",
        "colab_type": "code",
        "outputId": "a6f6acdd-51db-4899-abc6-aec75b1e605b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1148003, 40, 108)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZyzqkxykOfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(108, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRoTQ6ewmFo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-fmB2zakVo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(works) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = works[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    #sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        generated += next_char\n",
        "        \n",
        "        # sys.stdout.write(next_char)\n",
        "        # sys.stdout.flush()\n",
        "    print(generated)\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hz7u0QakWs1",
        "colab_type": "code",
        "outputId": "335f05b5-8dfe-4cf5-ef09-79716ac86711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=100,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1148003 samples\n",
            "Epoch 1/10\n",
            "1147400/1148003 [============================>.] - ETA: 0s - loss: 1.5852\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"hy hook! Most dangerous\n",
            "    Is that tem\"\n",
            "hy hook! Most dangerous\n",
            "    Is that temperling the ble streat,\n",
            "    By lose for a pett you his heards, make\n",
            "That hip help out baried weep the bare the Rome.\n",
            "  EPALCUS. Call, is she mely.\n",
            "  CONIALUS. Thou the truest alp your\n",
            "    asget you art thee an our seepticteling,\n",
            "As no’ friend an entreasumofils of will diships.\n",
            "    And my him was the priest afold the visce\n",
            "    Sever's be my serfrous his faft Mashing, ot propise\n",
            "    What is\n",
            "\n",
            "1148003/1148003 [==============================] - 82s 72us/sample - loss: 1.5852\n",
            "Epoch 2/10\n",
            "1147300/1148003 [============================>.] - ETA: 0s - loss: 1.5609\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"s else I would be, were the day prolong'\"\n",
            "s else I would be, were the day prolong's ding?\n",
            "  VILEND, ENjon]\n",
            "  For Terulio? Sir, tell him.\n",
            "  HOSTAND. Milowars which Bare the uncercideate.\n",
            "      Lordance!\n",
            "  POOT. Acworn, but repruater is, lefe's served epher,\n",
            "    Should be Damenio me, I prumal! there a\n",
            "offer where; thou, no sue, thyselves!\n",
            "Prof! These do beear.  The eard playerer; calogh?\n",
            "    I have yous Ricator.\n",
            "\n",
            "               here my good Torn and alodech.\n",
            "\n",
            "       \n",
            "\n",
            "1148003/1148003 [==============================] - 85s 74us/sample - loss: 1.5609\n",
            "Epoch 3/10\n",
            "1147600/1148003 [============================>.] - ETA: 0s - loss: 1.5429\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"ne earnest words,\n",
            "    Mine eyes should \"\n",
            "ne earnest words,\n",
            "    Mine eyes should blood upon the twallook,\n",
            "Yet him blaugh it us he isings me anvides.\n",
            "    To a proves and your with my todely book.\n",
            "  BROTUS. I she trues were more you leet,\n",
            "    Thus movel? Who, the sent first rosel for do to\n",
            "be a good in our ream:\n",
            "\n",
            "DRUNGO.\n",
            "Be the senter. I say,\n",
            "Thou ’t Will And, Kile Antoniomery.\n",
            "\n",
            "NEATHOLUS.\n",
            "To looked into were pleckle, as it:\n",
            "True not with him that then but her dusbe\n",
            "\n",
            "1148003/1148003 [==============================] - 83s 72us/sample - loss: 1.5430\n",
            "Epoch 4/10\n",
            "1148000/1148003 [============================>.] - ETA: 0s - loss: 1.5274\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \" like a fearful lad,\n",
            "    With tearful e\"\n",
            " like a fearful lad,\n",
            "    With tearful evench, he’s dosemppition;\n",
            "Ahill,—Inaments to! some his head, all she\n",
            "to-mingder; them, he breathe,\n",
            "She kingly and with your possipieds!\n",
            "Whither' duth your fash aft, once: I pronance was were maing\n",
            "And hast he hands be as thy everter pluck’d a base\n",
            "And their talbele of tenthing, this dear gues\n",
            "Is welt in law lov’d, in dirmen?\n",
            "\n",
            " Enter Part.\n",
            "LiO Alopper, and net green.\n",
            "\n",
            "GUMILIO.\n",
            "When mis\n",
            "\n",
            "1148003/1148003 [==============================] - 81s 71us/sample - loss: 1.5274\n",
            "Epoch 5/10\n",
            "1147700/1148003 [============================>.] - ETA: 0s - loss: 1.5137\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"ore and a cuckold—a good quarrel to draw\"\n",
            "ore and a cuckold—a good quarrel to drawd,\n",
            "Of overcharge him, dear which did,\n",
            "    Shress it I are from thy beatter!\n",
            "  CLARAND. I that inless the POONUSPERGA, and Mysels\n",
            "  CAOTERS. My, prest it thou sonart, any lands eyes would depold hast are versb\n",
            "    A cafforce to canst be that well may. Make yet ingent-mon?\n",
            "    Both droak, and be thou dace 'ristard your of the do;\n",
            "  Stupbles makinoment lated will-slander hears;\n",
            "That ear my lo\n",
            "\n",
            "1148003/1148003 [==============================] - 81s 71us/sample - loss: 1.5138\n",
            "Epoch 6/10\n",
            "1147300/1148003 [============================>.] - ETA: 0s - loss: 1.5019\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"derers._]\n",
            "\n",
            "It is concluded. Banquo, th\"\n",
            "derers._]\n",
            "\n",
            "It is concluded. Banquo, their renerely; and has,\n",
            "\n",
            "                            Enter AVOL9aNC\n",
            "\n",
            "  KING RICHARD. Gdom my rockepos'd offortion,\n",
            "    To that me talk think be sigh-compase.\n",
            "\n",
            " He place such o’ the groans sire, and call burast,\n",
            "A rage, darching and sucqualy;\n",
            "Which! but, let, thyself, thou hast desculan you.\n",
            "\n",
            "LEAR ENW.\n",
            "Awayes wronter.\n",
            "\n",
            "HERICLES.\n",
            "What lord? ’Tis wisskes th' kinds? I keems it, which I s\n",
            "\n",
            "1148003/1148003 [==============================] - 81s 71us/sample - loss: 1.5019\n",
            "Epoch 7/10\n",
            "1148000/1148003 [============================>.] - ETA: 0s - loss: 1.4915\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \" a ball;\n",
            "My words would bandy her to my\"\n",
            " a ball;\n",
            "My words would bandy her to my scrition:\n",
            "                  Exit BELARD SUFFOLK, 'Gidram Warwetts\n",
            "\n",
            "                Exeunt meet,\n",
            "\n",
            "\n",
            "Enter Payina.]\n",
            "\n",
            "CAKINGER.\n",
            "Cansout, forthing his mustly; by thou lov'd, I care I know he,\n",
            "    And such and compant inches take your Hereot.\n",
            "    Are thy triers him ayer'd out of termal\n",
            "    And up thee conned are speed up mightone?\n",
            "    Here is vastents before to the pallenment thee: I gaden\n",
            "\n",
            "1148003/1148003 [==============================] - 82s 71us/sample - loss: 1.4915\n",
            "Epoch 8/10\n",
            "1147300/1148003 [============================>.] - ETA: 0s - loss: 1.4818\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"! He will require them\n",
            "    As if he did\"\n",
            "! He will require them\n",
            "    As if he did I wange the men eastet\n",
            "    Which in the Coof Englenza for the great,\n",
            "  TIMON JOHNch; Brutus to BANCHUS. It majesterness confess that the thilst skinded.\n",
            "  HESS OF. The Kighmen. No, the signs, nor slifive a good, rumine;\n",
            "    Cresomely, and to tell me aviat his coughther,\n",
            "Though vair Buh madulf. Perish air 're god's did\n",
            "    It strance you his freathers are set the secreted as\n",
            "the name, we ki\n",
            "\n",
            "1148003/1148003 [==============================] - 81s 71us/sample - loss: 1.4818\n",
            "Epoch 9/10\n",
            "1147600/1148003 [============================>.] - ETA: 0s - loss: 1.4733\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"y!\n",
            "The sun is high, and we outwear the \"\n",
            "y!\n",
            "The sun is high, and we outwear the sin!\n",
            "  CILOTUS. Natting;\n",
            "    Their verts art this an one deat send\n",
            "    That to faiths, and desence my marron,\n",
            "    As swere, then me for tongy and for some\n",
            "    Of he? Yow, judgue not if they are, my creim.\n",
            "\n",
            "        'And Consampet, shood's comiction.\n",
            "  ROT. At atant Princosinia, give her fie that you,\n",
            "Loivoul fo ley be adfull'd me poum, and to?\n",
            "  MARGORDICM. Thy rest let answitient to Bisp\n",
            "\n",
            "1148003/1148003 [==============================] - 82s 72us/sample - loss: 1.4733\n",
            "Epoch 10/10\n",
            "1147700/1148003 [============================>.] - ETA: 0s - loss: 1.4657\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"gh,\n",
            "  That she will draw his lips’ rich\"\n",
            "gh,\n",
            "  That she will draw his lips’ rich ladd’d the\n",
            "ask) that thou angrager than like sathencion.\n",
            "    Our Lajues, POMPOS\n",
            "\n",
            " RBATBERYER ELES. Thou bearble madace;\n",
            "Brielus stol face you beright, sire, a some\n",
            "    as he shape instrumal and dissome or his\n",
            "    Juliegne of hime\n",
            "    And stain and men unstenne they maid,\n",
            "    Thou dintless on vanive in unall best attendsh;\n",
            "Droke sendied in are quomin perform,\n",
            "To remund than fet their ea\n",
            "\n",
            "1148003/1148003 [==============================] - 83s 72us/sample - loss: 1.4657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa93e718b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}